{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **A Simple Image Classifier with MobileNet.**\n",
        "\n",
        "* to directly run the model please run the first cell and directly skip to the last cell\n",
        "\n"
      ],
      "metadata": {
        "id": "F7ho5X7durlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning the repository here and getting the required data for training and testing\n",
        "!git clone https://github.com/theUnrealSamurai/simple_classifier.git"
      ],
      "metadata": {
        "id": "RM447DBCnqsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6f9dd1-5a9b-4026-86d1-259dc001b222"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'simple_classifier'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
            "remote: Compressing objects: 100% (221/221), done.\u001b[K\n",
            "remote: Total 225 (delta 3), reused 224 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (225/225), 16.65 MiB | 21.50 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries here and setting the random seed to generate\n",
        "# the same output results everytime. If the random seed is not set,\n",
        "# the performance of the model may vary everytime you train it.\n",
        "\n",
        "\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    pd.np.random.seed(seed)\n",
        "    sklearn.utils.check_random_state(seed)\n",
        "\n",
        "set_random_seed(13)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB9PKOszsjqW",
        "outputId": "755fd252-af3e-413b-bc3d-d81efc7c353f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-37b209f44fc4>:16: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
            "  pd.np.random.seed(seed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RkaxEWD76MFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2800c9ac-9e70-46bc-ebf5-595faf2a0ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 174 images belonging to 7 classes.\n",
            "Found 16 images belonging to 7 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17225924/17225924 [==============================] - 0s 0us/step\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 60s 7s/step - loss: 0.5138 - accuracy: 0.8161 - val_loss: 0.7513 - val_accuracy: 0.8750\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 44s 8s/step - loss: 0.0137 - accuracy: 0.9943 - val_loss: 1.1554 - val_accuracy: 0.8750\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 43s 7s/step - loss: 0.0137 - accuracy: 0.9943 - val_loss: 0.8561 - val_accuracy: 0.8750\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 42s 7s/step - loss: 0.0064 - accuracy: 0.9943 - val_loss: 0.3066 - val_accuracy: 0.9375\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 44s 8s/step - loss: 6.6014e-04 - accuracy: 1.0000 - val_loss: 0.4694 - val_accuracy: 0.9375\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 43s 7s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1475 - val_accuracy: 0.9375\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 47s 8s/step - loss: 4.9289e-04 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 42s 7s/step - loss: 4.8860e-04 - accuracy: 1.0000 - val_loss: 2.9854e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 43s 7s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.4077e-04 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# path to the folder where the training images are saved.\n",
        "data_folder = '/content/simple_classifier/data/train_data'\n",
        "\n",
        "# Defining Image size and batch size here.\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.1 # Change the size of the validation dataset here.\n",
        "                         # Since the size of the dataset is small I'm using 0.1\n",
        ")\n",
        "\n",
        "# Generating the Training dataset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_folder,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Generating the Validation dataset\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_folder,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Getting the pretrained mobile net model to finetune it for our dataset.\n",
        "base_model = MobileNet(include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Modifying the output nodes to fit for only 7 classes that we have.\n",
        "# i.e [bar, cherry, elephant, clown, z, grape, bell]\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Training and saving the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 9\n",
        "model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
        "\n",
        "model.save('trained_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to TFLite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model to a file\n",
        "with open('classifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYKT114tn2k_",
        "outputId": "807f84a9-79e9-435f-9a10-c7f63b09b7d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/classifier.tflite /content/drive/MyDrive/trained_model\n",
        "!cp /content/trained_model.h5 /content/drive/MyDrive/trained_model"
      ],
      "metadata": {
        "id": "geWYAFWfYFOh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To test for any other sample pictures please upload them in the environment\n",
        "# and give the path to the image as a parameter to the inference.py file.\n",
        "\n",
        "# there are 10 more available pictures in the\n",
        "# /content/simple_classifier/data/test_data directory please use the file\n",
        "# explorer to find and test them. And double click on them to view the file.\n",
        "\n",
        "!python3 /content/simple_classifier/inference.py /content/simple_classifier/data/test_data/1.png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ1BFll9YFL9",
        "outputId": "24b90887-8a5d-42a9-e191-0ff627b8f9c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-09 17:51:40.209046: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-09 17:51:41.659207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "Figure(640x480)\n",
            "Image split into 3 pieces successfully.\n",
            "{'accuracy': 0.99991846, 'class_index': 5, 'class': 'grape'}\n",
            "{'accuracy': 0.9997311, 'class_index': 5, 'class': 'grape'}\n",
            "{'accuracy': 0.99833906, 'class_index': 4, 'class': 'elephant'}\n",
            "None\n",
            "ran in 0.4371216297149658 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etzTvL65xspg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}